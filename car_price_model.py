# -*- coding: utf-8 -*-
"""car_price_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1stCDtdZ_sr7kUAHYB8c7fzg5F02H20qI
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""## Data Gathering"""

df = pd.read_csv("car data.csv")
df.head()

"""## Data Preparation"""

df.info()

df.shape

df.columns

df.isnull().sum()

#no null values

df.describe()

df['Fuel_Type'].unique()

df['Seller_Type'].unique()

df = df.drop('Car_Name',axis = 'columns')

df.head()

"""## Feature Engineering"""

df['current_year'] = 2021
df.head()

#we can get car age -> current_year - Year

df['Age'] = df['current_year'] - df['Year']
df.head()

df.shape

df.drop('Year',axis = 'columns',inplace =True)

df.head()

df.shape

final_df = pd.get_dummies(df,drop_first=True)
final_df.head()

#all the string type variables have been converted into numerical type data

"""
### Fuel_Type feature:
Fuel is Petrol if Fuel_type_diesel = 0 ,Fuel_Type_Petrol = 1
Fuel is Diesel if Fuel_type_diesel = 1 ,Fuel_Type_Petrol = 0
Fuel is cng if Fuel_type_diesel = 0 ,Fuel_Type_Petrol = 0|
### Transmission feature:
transmission is manual if Transmission_Manual = 1
transmission is automatic if Transmission_Manual = 0
### Seller_Type feature:
Seller_Type is Individual if Seller_Type_Individual = 1
 Seller_Type is dealer if Seller_Type_Individual = 0"""

final_df.drop('current_year',axis = 1,inplace = True) #dropping current_year becoz it shows NaN correlation
final_df.corr()

"""### Pairplot"""

import seaborn as sns
sns.pairplot(df)

#create correlation matrix
corr = final_df.corr()
indx=corr.index

#plot this correlation for clear visualisation
plt.figure(figsize=(13,11))
#annot = True , dsiplays text over the cells.
#cmap = "YlGnBu" is nothing but adjustment of colors for our heatmap
sns.heatmap(final_df[indx].corr(),annot=True,cmap="Blues")
#amount of darkness shows how our features are correlated with each other

"""### Feature and Target Variable"""

# x and y split
X = final_df.iloc[:,1:]
Y = final_df.iloc[:,0] #we want to predict selling price

X.head()

Y.head()

"""##Checking Feature importance"""

from sklearn.ensemble import ExtraTreesRegressor

model = ExtraTreesRegressor()
model.fit(X,Y)
model.feature_importances_

#feature importance visualization

feat_imp = pd.Series(model.feature_importances_,index = X.columns)

feat_imp.nlargest(5).plot(kind = 'barh') #top 5 important

"""### Train-Test Splitting our dataset"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=8)

"""## Fitting and evaluating different models

1. Linear Regression
2. Decision Tree
3. Random forest Regressor

### Linear Regression
"""

from sklearn.linear_model import LinearRegression
model1 = LinearRegression()
model1.fit(X_train,Y_train)
Y_pred = model1.predict(X_test)
Y_pred

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
print("MAE:",mean_absolute_error(Y_pred,Y_test))
print("MSE:",mean_squared_error(Y_pred,Y_test))
print("RMSE:",np.sqrt(mean_squared_error(Y_pred,Y_test)))
print("R2 Score:",r2_score(Y_pred,Y_test))

"""## Decision Tree"""

from sklearn.tree import DecisionTreeRegressor
model2 = DecisionTreeRegressor()
model2.fit(X_train,Y_train)
Y_pred = model2.predict(X_test)
Y_pred

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
print("MAE:",mean_absolute_error(Y_pred,Y_test))
print("MSE:",mean_squared_error(Y_pred,Y_test))
print("RMSE:",np.sqrt(mean_squared_error(Y_pred,Y_test)))
print("R2 Score:",r2_score(Y_pred,Y_test))

"""## Random Forest Algorithm"""

from sklearn.ensemble import RandomForestRegressor
model3 = RandomForestRegressor()
model3.fit(X_train,Y_train)
Y_pred = model3.predict(X_test)
Y_pred

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
print("MAE:",mean_absolute_error(Y_pred,Y_test))
print("MSE:",mean_squared_error(Y_pred,Y_test))
print("RMSE:",np.sqrt(mean_squared_error(Y_pred,Y_test)))
print("R2 Score:",r2_score(Y_pred,Y_test))

"""We want our R2 score to be maximum and other errors to be minimum for better results

We can also do the same using K-Folds method

### Random forest regressor is giving better results. therefore we will hypertune this model and then fit, predict.

## Hyperparameter tuning
"""

#The number of trees in the forest
n_estimator = [int(x) for x in np.linspace(start = 100,stop = 1200,num = 12)]
n_estimator

from sklearn.model_selection import RandomizedSearchCV

#we are creating a dictionary of parameters

n_estimators = [int(x) for x in np.linspace(start = 100,stop = 1200,num = 12)]
max_features = ['auto','sqrt'] # Number of features to consider at every split
max_depth = [int(x) for x in np.linspace(5,30,num=6)]# Maximum number of levels in tree
min_samples_split = [2,5,10,15,100]# Minimum number of samples required to split a node
min_samples_leaf = [1,2,5,10] # Minimum number of samples required at each leaf node

#after creating the different lists, we need to create a dictionary like this
#create th random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
                'min_samples_split' : min_samples_split,
                'min_samples_leaf':min_samples_leaf }
random_grid

#create our moodel to tune
# then use random grid to search for the best hyperparameter

# random_grid has all our parameters
final = RandomForestRegressor()
final_random = RandomizedSearchCV(estimator=final,param_distributions=random_grid,scoring='neg_mean_squared_error',n_iter=100,cv = 5,verbose = 2,random_state=2,n_jobs = 1)

final_random.fit(X_train,Y_train)

#property of randomSearchCV
final_random.best_params_

final_random.best_score_

"""### Final Predictions"""

Y_pred = final_random.predict(X_test)
Y_pred

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
print("MAE:",mean_absolute_error(Y_pred,Y_test))
print("MSE:",mean_squared_error(Y_pred,Y_test))
print("RMSE:",np.sqrt(mean_squared_error(Y_pred,Y_test)))
print("R2 Score:",r2_score(Y_pred,Y_test))

"""## Saving the model"""

# import pickle
# # open a file, where you ant to store the data
# file = open('car_price_model.pkl', 'wb')

# # dump information to that file
# pickle.dump(final_random, file)

